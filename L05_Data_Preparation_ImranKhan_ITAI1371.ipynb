{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyFzWIw_rN-s"
      },
      "source": [
        "# Module 05 Lab - Data Preparation**Objective:** To learn and apply the most common data preparation techniques. Raw data is rarely ready for a machine learning model. This process, also called preprocessing, is one of the most critical steps in the entire ML workflow.**In this lab, you will write more of the code.** Read the explanations and then complete the tasks in the code cells."
      ],
      "id": "VyFzWIw_rN-s"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d_uImF4rN-u"
      },
      "source": [
        "## Part 1: Setup and Initial LookWe will continue using the Titanic dataset because it has the exact problems we need to solve: missing values and non-numeric data."
      ],
      "id": "4d_uImF4rN-u"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
        "\n",
        "# Let's look at the missing values\n",
        "print(\"--- Missing Values Before Cleaning ---\")\n",
        "print(df.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmh4B3ysr-oI",
        "outputId": "03ce17c0-8391-470b-f1d8-9fe69c8189e2"
      },
      "id": "lmh4B3ysr-oI",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Missing Values Before Cleaning ---\n",
            "PassengerId      0\n",
            "Survived         0\n",
            "Pclass           0\n",
            "Name             0\n",
            "Sex              0\n",
            "Age            177\n",
            "SibSp            0\n",
            "Parch            0\n",
            "Ticket           0\n",
            "Fare             0\n",
            "Cabin          687\n",
            "Embarked         2\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Iz6m0JGrN-u"
      },
      "source": [
        "## Part 2: Handling Missing Values (Imputation)**Concept:** Most machine learning models cannot handle missing values (`NaN`). We must deal with them. Dropping the rows is an option, but you lose data. A better way is **imputation**, which means filling in the missing values with a calculated guess.Common imputation strategies:*   **Mean:** Fill with the average value. Good for normally distributed data.*   **Median:** Fill with the middle value. Better for skewed data or data with outliers (like `Fare`).*   **Mode:** Fill with the most frequent value. Used for categorical data."
      ],
      "id": "7Iz6m0JGrN-u"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TneFTf8KrN-u"
      },
      "source": [
        "### Task 1: Impute the 'Age' ColumnThe 'Age' column is missing many values. Since age can be skewed (e.g., by a few very old passengers), using the **median** is a robust choice.**Your Task:** Calculate the median of the 'Age' column and use the `.fillna()` method to replace the missing values."
      ],
      "id": "TneFTf8KrN-u"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-ZUMN6ZrN-u",
        "outputId": "000516ee-8205-4f85-eab0-3adbcc169d86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in 'Age' after imputation:\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2888799683.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Age'].fillna(median_age, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# 1. Calculate the median of the 'Age' column\n",
        "median_age = df['Age'].median()\n",
        "\n",
        "# 2. Fill the missing values in 'Age' with the median\n",
        "df['Age'].fillna(median_age, inplace=True)\n",
        "\n",
        "# 3. Verify that there are no more missing values in 'Age'\n",
        "print(\"Missing values in 'Age' after imputation:\")\n",
        "print(df['Age'].isnull().sum())\n"
      ],
      "id": "4-ZUMN6ZrN-u"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLGeniRzrN-u"
      },
      "source": [
        "## Part 3: Encoding Categorical Features**Concept:** Machine learning models are mathematical, so they need numbers, not text. We need to convert categorical columns (like 'Sex' and 'Embarked') into a numerical format. The most common method is **One-Hot Encoding**.One-Hot Encoding takes a column with `N` categories and turns it into `N` new columns, each with a `1` or `0`. For example, the 'Sex' column (`male`, `female`) becomes two new columns: `Sex_male` and `Sex_female`.Pandas has a convenient function called `pd.get_dummies()` that does this for us."
      ],
      "id": "NLGeniRzrN-u"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT92ZsgirN-v"
      },
      "source": [
        "### Task 2: One-Hot Encode Categorical Columns**Your Task:** Use `pd.get_dummies()` to encode the 'Sex' and 'Embarked' columns. Make sure to drop the original columns after encoding."
      ],
      "id": "iT92ZsgirN-v"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD_BpWMxrN-v",
        "outputId": "26f49d7b-b9d5-4406-8341-6cb32032b686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name   Age  SibSp  Parch  \\\n",
            "0                            Braund, Mr. Owen Harris  22.0      1      0   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0      1      0   \n",
            "2                             Heikkinen, Miss. Laina  26.0      0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0      1      0   \n",
            "4                           Allen, Mr. William Henry  35.0      0      0   \n",
            "\n",
            "             Ticket     Fare Cabin  Sex_male  Embarked_Q  Embarked_S  \n",
            "0         A/5 21171   7.2500   NaN      True       False        True  \n",
            "1          PC 17599  71.2833   C85     False       False       False  \n",
            "2  STON/O2. 3101282   7.9250   NaN     False       False        True  \n",
            "3            113803  53.1000  C123     False       False        True  \n",
            "4            373450   8.0500   NaN      True       False        True  \n"
          ]
        }
      ],
      "source": [
        "# 1. Use get_dummies to create new columns for 'Sex' and 'Embarked'\n",
        "encoded_df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "# 2. Display the first few rows of the new DataFrame to see the new columns\n",
        "print(encoded_df.head())\n"
      ],
      "id": "UD_BpWMxrN-v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bStmA-BzrN-v"
      },
      "source": [
        "## Part 4: Feature Scaling**Concept:** Many models are sensitive to the scale of the features. For example, `Age` (from 0-80) and `Fare` (from 0-512) are on very different scales. This can cause the model to incorrectly believe that `Fare` is a more important feature simply because its values are larger.**Feature Scaling** solves this by putting all features on a similar scale. A common method is **Standardization** (`StandardScaler` in scikit-learn), which rescales the data to have a mean of 0 and a standard deviation of 1.**Important:** You only scale your numerical features, not your target variable or your newly encoded categorical columns."
      ],
      "id": "bStmA-BzrN-v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOF0Z6QFrN-v"
      },
      "source": [
        "### Task 3: Scale the 'Age' and 'Fare' Columns**Your Task:** Use `StandardScaler` from `sklearn.preprocessing` to scale the 'Age' and 'Fare' columns."
      ],
      "id": "XOF0Z6QFrN-v"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zukc2pc3rN-v",
        "outputId": "96e48454-0105-4b42-fa8e-d4aa6cd154ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Age      Fare\n",
            "0 -0.565736 -0.502445\n",
            "1  0.663861  0.786845\n",
            "2 -0.258337 -0.488854\n",
            "3  0.433312  0.420730\n",
            "4  0.433312 -0.486337\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Create an instance of the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 2. Select the columns to scale\n",
        "columns_to_scale = ['Age', 'Fare']\n",
        "\n",
        "# 3. Fit the scaler to the data and transform it\n",
        "encoded_df[columns_to_scale] = scaler.fit_transform(encoded_df[columns_to_scale])\n",
        "\n",
        "# 4. Display the first few rows to see the scaled data\n",
        "print(encoded_df[columns_to_scale].head())\n"
      ],
      "id": "zukc2pc3rN-v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qln3gMXVrN-v"
      },
      "source": [
        "## Knowledge Checkpoint Instructions: Answer the following questions in this markdown cell.\n",
        "\n",
        "1. **Why is it often better to impute missing values with the median instead of the mean?**\n",
        "\n",
        "The median is more robust to outliers than the mean. If there are extreme ages in the dataset, the mean can be pulled away from the central tendency, while the median remains a typical representative value.\n",
        "\n",
        "2. **Explain in your own words what One-Hot Encoding does and why it is necessary.**\n",
        "\n",
        "One-Hot Encoding converts categorical variables (like Sex and Embarked) into separate binary columns (0 or 1) for each category. This is necessary because most machine learning algorithms require numeric inputs and cannot directly process text labels.\n",
        "\n",
        "3. **Would you need to apply Feature Scaling to a Decision Tree model? Why or why not? (Hint: Think about how a Decision Tree makes its splits).**\n",
        "\n",
        "You do not need to apply Feature Scaling to a Decision Tree model. Decision Trees make splits based on feature thresholds and the ordering of values, not on the absolute magnitude of features, so scaling does not affect how the tree makes its decisions."
      ],
      "id": "qln3gMXVrN-v"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}